---
title: "Star Trek: The Analysis"
author: "Satvik Kishore"
output:
  pdf_document: default
  html_notebook: default
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)

library(data.table)
library(rjson)
library(magrittr)
library(ggplot2)
library(lme4)
library(xtable)

shows = c("TNG", "DS9", "VOY", "ENT")
maincast = list()
maincast$TNG = c("PICARD", "RIKER", "LAFORGE", "WORF", "CRUSHER", "TROI", "DATA", "WESLEY")
maincast$DS9 = c("SISKO", "ODO", "DAX", "JAKE", "OBRIEN", "QUARK", "BASHIR", "KIRA", "WORF")
maincast$VOY = c("JANEWAY", "CHAKOTAY", "TUVOK", "PARIS", "EMH", "TORRES", "KIM", "NEELIX", "KES", "SEVEN")
maincast$ENT = c("ARCHER", "TPOL", "PHLOX", "REED", "TRAVIS", "HOSHI", "TUCKER")

dat = readRDS("../data/from_imdb_sow.rds")
s3 = readRDS("../data/cast_count.rds")
dat$director_f = as.factor(dat$director)
dat$show_f = factor(dat$show, levels = shows)
dat$season_f = factor(paste(dat$show, dat$seasonNumber, sep=":"))
names(dat)[names(dat)=="cast_T'POL"] = "cast_TPOL"
dat[,n_ep_in_season := .N, by=season_f]
dat[,episode_placement := episodeNumber/n_ep_in_season]
dat[episode_placement < 0.33, partofseason := "start"]
dat[episode_placement < 0.66 & episode_placement >= 0.33, partofseason := "middle"]
dat[episode_placement >= 0.66, partofseason := "end"]

```

## Summary  

Star Trek is Science Fiction media franchise spanning several TV shows, movies, books, video games, and more. In this analysis, I focused on four Star Trek TV shows that ran in sequence from 1987 through 2005, with significant overlaps in the years they each ran. The questions I wanted to answer are:  

- Are their differences in quality across the four shows and their constituent seasons
- Which characters had the greatest influence on the rating of the episode, either positive or negative
- Are their differences in quality across the directors

To answer these questions, I used IMDb ratings of the episodes. I performed hierarchical linear regressions and answered my questions using the estimates from this model. The most important results were %%%%TODO%%%%  

## Introduction  

Star Trek is one of the most successful science fiction media franchises and was an instant hit with its inception in the 1960s. The series continues to be relevant, with most of the TV shows being available on Over-the-top services like Netflix. As of December 2021, there are three TV shows running, with one another set to begin in 2021. In my analysis, I have chosen the shows The Next Generation, Deep Space 9, Voyager, and Enterprise. These four shows ran from 1987 through 2005 and constitute the largest block of content produced by the franchise. I have omitted The Original Series as well as the newer because of the separation in time between them. In contrast, these four shows had a similar system, with a consistent set of personnel behind their production. The shows are mostly episodic in format, i.e. each episode is self contained within its plot, having an introduction, body, and conclusion. The episodes are not completely independent however, with plot elements sometimes having an effect on the larger story. There are a few multi-parter episodes as well. The shows each have a set of core characters, that have varying degrees of screentime in each episode. In addition there are minor recurring characters and usually a few one-time. In the public, there are varying levels of opinions on the qualities of individual core characters, but there are a clear few fan-favorites and some that are widely disliked. My first question pertains to these perceptions, and my question is on how character screentimes were associated with episode ratings, and whether my findings align with opinions found elsewhere. As is the case with TV shows, there are many directors, and the quality of directors is expected to have a strong effect on the quality of the episodes. Therefore, another research question of mine was does the quality of directors differ, and if yes, which were the best (or worst). My third question was on differences in quality between the shows, and their constituent seasons. 

## Data  

The two sources of data are the IMDb datasets provided IMDb on datasets.imdbws.com (collected on Nov 23, 2021), and script data provided on www.chakoteya.net/StarTrek/index.html. The IMDb data was available in a relational schema form. It was very large and had most of IMDb data contained. I filtered and wrangled to select only the four shows in consideration, with the variables: director name, names of writers, episode number, season number, and the IMDb rating. A few episodes had multiple directors. Combination of directors were considered to be a different director. Most episodes had multiple writers, and this variable was dropped from further analysis. The script data was provided as a json, formatted as Show -> episode -> character -> all lines. For each character, within each show, I computed total number of words spoken by the character in the episode, and divided it by the total number of words spoken in the entire episode. This gave me the screentime for each character in each episode as a percentage with the value being between 0 and 100. I only picked main characters from this transformed data and merged it with the IMDb data. There were a few issues with matching episodes as the scripts data sometimes assumed multi-part episodes as single episodes. These episodes were split before merging, and the final data assumes same screentime values for each episode in these multi-part episodes to be the same. 

### Exploratory Data Analysis:  

```{r data_eda1, results='asis', out.width='50%'}
t1 = data.frame(Show = c("The Next Generations","Deep Space 9","Voyager","Enterprise","Total"),
                Abbrv. = c(shows,""),
                n_seasons = c(7,7,7,4, 25),
                n_episodes = c(176,173,168,97, 614),
                n_directors = c(38, 33, 33, 20, 73),
                n_characters = c(8, 9, 10, 7, 33))
print(xtable(t1, digits = 0, caption = "Data Summary", ), comment=FALSE, include.rownames=FALSE)

ggplot(dat) + aes(imdb) + geom_histogram() +
  scale_x_continuous(breaks = (0:5)*2) + 
  xlim(c(3,10)) +
  xlab("IMDB Rating") +
  ggtitle("Histogram of Ratings across episodes", subtitle = "Figure 1") +
  theme_classic()

char_powwer = function(sh){
  out = data.table(show = sh, character = names(s3[[sh]]), count = s3[[sh]])
  out$corr = sapply(names(s3[[sh]]), function(z) cor(dat[show==sh]$imdb, dat[show==sh][[paste0("cast_",z)]]) )
  out
}
eda_char_pow = lapply(shows, char_powwer) %>% do.call(rbind,.)

ggplot(eda_char_pow) + aes(x = paste(show,character,sep=":"), y = count, fill = corr) +
  geom_col(color = "black") + 
  theme_classic() + 
  scale_fill_gradient2(low = "red", mid = "#EEEEAA", high = "blue", midpoint = 0, name = "Correlation") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) + 
  xlab("Character") + ylab("Percentage Screentime in repective show") + 
  ggtitle("Correlation with IMDb Rating and percetage sceentime of characters", subtitle = "Figure 2")
  
```

```{r data_eda2, out.width="50%"}
ggplot(dat) + aes(x=show_f, y=imdb) +
  geom_boxplot() + 
  theme_classic() +
  xlab("SHOW") + ylab("IMDb Rating") + 
  ylim(c(3.5, 9.5)) +
  scale_y_continuous(breaks = 1:10, limits = c(3.24, 9.55)) +
  ggtitle("Boxplot of ratings by shows", subtitle = "Figure 3")
ggplot(dat[show=="TNG"]) + aes(x=season_f, y=imdb) +
  geom_boxplot() +
  theme_classic() +
  scale_y_continuous(breaks = 1:10, limits = c(3.24, 9.55)) +
  xlab("Season") + ylab("IMDb Rating") + 
  ggtitle("Boxplot of ratings by seasons in TNG", subtitle = "Figure 4")
```
Table 1 provides a information on structure of the data, including counts of variables. Figure 1 illustrates the distribution of episode ratings. The distribution appeared to be roughly normal, permitting further analysis without variable transformations. Figure 2 illustrates the correlation of character-screentime with the IMDb ratings, along with what percentage of screentime the character occupied in their respective show. We observed some strong negative correlations for "TNG: Troi" and "DS9: Dax", while seeing strong positive correlation for characters like "DS9: Sisko" and "TNG: Picard". Figure 3 illustrates distributions of ratings within each show. The shows appeared to have similar ratings, with the median of TNG being a little lower than the medians of others. Figure 4 shows the boxplots of ratings across seasons within TNG. We observed significant variations across seasons, with the later seasons being somewhat better rated. We observed similar trends for other shows as well (not illustrated).  

## Modeling    
```{r basemodel}
# m_onlyseason = lmer(formula = imdb ~  (1|season_f), data = dat)
# 
# m_only_director = lmer(formula = imdb ~  (1|director_f), data = dat)
# 
# m_onlyheirarchics = lmer(formula = imdb ~  (1|season_f) + (1|director_f), data = dat)
# 
# f_withpos = paste(names(dat)[grepl("cast",names(dat))], collapse = " + ")
# f_withpos = paste("imdb ~",f_withpos, " + partofseason + (1|season_f) + (1|director_f)") %>% as.formula
# m_withpos = lmer(formula = f_withpos, data = dat)

f_regular = paste(names(dat)[grepl("cast",names(dat))], collapse = " + ")
f_regular = paste("imdb ~",f_regular, " + (1|season_f) + (1|director_f)") %>% as.formula
m_regular = lmer(formula = f_regular, data = dat)
sm_regular = summary(m_regular)
re_regular=unclass(VarCorr(m_regular))
re_regular = c("season" = re_regular$season_f[[1]], 
               "director" = re_regular$director_f[[1]], 
               "residual" = attr(re_regular,"sc")^2)
re_regular_perc = 100*re_regular/sum(re_regular)
re_regular_perc %<>% round(2)

# f_onlycast = paste(names(dat)[grepl("cast",names(dat))], collapse = " + ")
# f_onlycast = paste("imdb ~",f_onlycast) %>% as.formula
# m_onlycast = lm(formula = f_onlycast, data = dat)
# 
# f_withshow = paste(names(dat)[grepl("cast",names(dat))], collapse = " + ")
# f_withshow = paste("imdb ~",f_withshow, " + (1|season_f) + (1|director_f) + (1|show_f)") %>% as.formula
# m_withshow = lmer(formula = f_withshow, data = dat)



#with episode counter to balance viewership 
#other potential models

#anova results
#regular = withpos
#regular > onlydirector
#regular > onlyheirarchichs
#regular > onlycast
#regular !! withshow
```
I used a linear hierarchical model to answer our research questions. The equation of the final model was:  

$$ y_i = \beta_0 + \gamma_{0,j} + \gamma_{1,k} + \sum_{p=1}^{33} \beta_px_{p,i} + \epsilon_i $$  

$$(\epsilon, \gamma_{0,j}, \gamma_{1,k}) \sim \mathcal{N}_3(0, \Sigma)$$

where $i$ is the observation index, $y$ is the IMDb rating, $\beta_0$ is the grand intercept of the model, $\gamma_{0,j}$ is the random intercept effect from the show:season, $\gamma_{1,k}$ is the random intercept effect from the director, $p$ is index for one of the 33 characters, $\beta_p$ is the linear coefficient for the screentime of character $p$, $x_{p,i}$ is the screentime value of character $p$ in episode $i$, $\epsilon_i$ is the residual, and $\Sigma$ is the covariance matrix for the three residual terms. I did not use random effects model as the current model had a quite a large number of variables, especially for the small data, and introduction of more random effects made the model too complex while hurting interpretability. I compared this model to a few others using anova. It was observed to be significantly better than a model with only director and season:show variables, indicating that the cast variables are important. It was also significantly better than a model without any of the random effects, indicating that the levels are important. It was not significantly different than the model which included position of episode within season as a fixed effect variable, and hence that variable was dropped. It was also not significantly different from the model that has show as another random effect, and hence that was also dropped.  

### Results  

In the random effects, the show:season level captured `r re_regular_perc["season"]`% of the variation. The director level captured `r re_regular_perc["director"]`% of the variation.





<!-- Summary -->
<!-- A few sentences describing the inferential question(s), the method used and the most important results. -->

<!-- Introduction -->
<!-- A more in-depth introduction to the inferential question(s) of interest. -->

<!-- Data -->
<!-- You should describe the data in this section: how you obtained the data, the variables included, dealing with missing/erroneous values, exploratory data analysis etc. -->

<!-- Model -->
<!-- A detailed description of the model used, how you selected the model, how you selected the variables, model assessment, model validation, and presentation of the model results. What are your overall conclusions in context of the inferential problem(s)? -->

<!-- Conclusions -->
<!-- In this section, you should present the importance of your findings, and describe any limitations of the study. You can also address future work here if there are extensions of your analysis you find interesting, especially those that may address some of the limitations already mentioned. -->

